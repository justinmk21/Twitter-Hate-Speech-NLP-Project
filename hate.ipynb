{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32228e22",
   "metadata": {},
   "source": [
    "## **Detecting Hate Speech on Twitter using NLP and Machine Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20aa37",
   "metadata": {},
   "source": [
    "#### **Project Overview**\n",
    "\n",
    "**Twiiter** is a massive social media platform where users can freely express their opinions. Unfortunately, some users spread hate speech, which can harm communities and individuals.\n",
    "\n",
    "In this project, we aim to automaitically detect speech in tweets using **Natural Language Processing** (NLP) and **Machine Learning** techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0d85d1",
   "metadata": {},
   "source": [
    "We will:\n",
    "- Clean and preprocess tweet text.\n",
    "- Convert into numerical features using **TF_IDF**.\n",
    "- Build a **Logical Regression** model.\n",
    "- Handle **class imbalance**\n",
    "- Perform **hyperparameter tuning** using **GridSearchCv** with **Stratified K-Fold** cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21040acb",
   "metadata": {},
   "source": [
    "## üìÇ Dataset\n",
    "**Columns:**\n",
    "- `id` -> Unique identifier for the tweet\n",
    "- `label` -> 0 = Non-hate, 1 = Hate speech.\n",
    "- `tweet` -> The actual tweet text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a2b868b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\KgomotsoMkhawane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\KgomotsoMkhawane\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d367213",
   "metadata": {},
   "source": [
    "#### **Basis Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc97a556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id  label                                              tweet\n",
      "0    1      0   @user when a father is dysfunctional and is s...\n",
      "1    2      0  @user @user thanks for #lyft credit i can't us...\n",
      "2    3      0                                bihday your majesty\n",
      "3    4      0  #model   i love u take with u all the time in ...\n",
      "4    5      0             factsguide: society now    #motivation\n",
      "5    6      0  [2/2] huge fan fare and big talking before the...\n",
      "6    7      0   @user camping tomorrow @user @user @user @use...\n",
      "7    8      0  the next school year is the year for exams.√∞¬ü¬ò...\n",
      "8    9      0  we won!!! love the land!!! #allin #cavs #champ...\n",
      "9   10      0   @user @user welcome here !  i'm   it's so #gr...\n",
      "10  11      0   √¢¬Ü¬ù #ireland consumer price index (mom) climb...\n",
      "11  12      0  we are so selfish. #orlando #standwithorlando ...\n",
      "12  13      0  i get to see my daddy today!!   #80days #getti...\n",
      "13  14      1  @user #cnn calls #michigan middle school 'buil...\n",
      "14  15      1  no comment!  in #australia   #opkillingbay #se...\n",
      "15  16      0  ouch...junior is angry√∞¬ü¬ò¬ê#got7 #junior #yugyo...\n",
      "16  17      0  i am thankful for having a paner. #thankful #p...\n",
      "17  18      1                             retweet if you agree! \n",
      "18  19      0  its #friday! √∞¬ü¬ò¬Ä smiles all around via ig use...\n",
      "19  20      0  as we all know, essential oils are not made of...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31962 entries, 0 to 31961\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      31962 non-null  int64 \n",
      " 1   label   31962 non-null  int64 \n",
      " 2   tweet   31962 non-null  object\n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 749.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "\n",
    "hate_speech = pd.read_csv('TwitterHate.csv')\n",
    "\n",
    "print(hate_speech.head(20))\n",
    "print(hate_speech.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "157d8a01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "da4beb23-51bd-4053-988a-8d05994cceb2",
       "rows": [
        [
         "id",
         "0"
        ],
        [
         "label",
         "0"
        ],
        [
         "tweet",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3
       }
      },
      "text/plain": [
       "id       0\n",
       "label    0\n",
       "tweet    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2445b3ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hate_speech.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117afff",
   "metadata": {},
   "source": [
    "#### **Convert Tweets to a List**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "01effae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = hate_speech['tweet'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856b1afd",
   "metadata": {},
   "source": [
    "#### **Text Cleaning**\n",
    "\n",
    "We will:\n",
    "- Lowercase text\n",
    "- Remove user handlings (@username)\n",
    "- Remove redundant terms (amp, rt)\n",
    "- Remove # but keep the word\n",
    "- Remove single-character terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db4c1af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tweet before cleaning:  @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run\n",
      "Sample cleaned tweet: ['father', 'dysfunctional', 'selfish', 'drags', 'kids', 'dysfunction', 'run']\n"
     ]
    }
   ],
   "source": [
    "# Display before cleaning\n",
    "print(\"Sample tweet before cleaning:\", tweets[0])\n",
    "\n",
    "# Initialize Tokenizer\n",
    "tokenizer = TweetTokenizer(\n",
    "    preserve_case=False, # Lowercase text\n",
    "    strip_handles=True, # Remove user handles\n",
    "    reduce_len=True # Reduce redundant terms (e.g. \"sooo\" to \"so\")\n",
    ")\n",
    "\n",
    "# Stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "redundant_words = {'amp', 'rt'}\n",
    "\n",
    "cleaned_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    # Remove URLs\n",
    "    tweet = re.sub(r'http\\S+|www\\S+|https\\s+', '', tweet, flags=re.MULTILINE)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    # Remove stopwords, redundant words, hashtags, and single characters\n",
    "    tokens = [re.sub(r'#', '', word) for word in tokens] # remove #\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in redundant_words and len(word) > 1] # remove stopwords, redundant words, and single characters\n",
    "\n",
    "    cleaned_tweets.append(tokens)\n",
    "\n",
    "print(\"Sample cleaned tweet:\", cleaned_tweets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88f9b24",
   "metadata": {},
   "source": [
    "#### **Check Top 10 Most Common Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2d455168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('...', 2808), ('love', 2748), ('day', 2276), ('happy', 1684), ('time', 1131), ('life', 1118), ('like', 1047), ('today', 1013), ('new', 994), ('thankful', 946), ('positive', 931), ('get', 917)]\n"
     ]
    }
   ],
   "source": [
    "all_words = [word for tokens in cleaned_tweets for word in tokens]\n",
    "word_freq = Counter(all_words)\n",
    "print(word_freq.most_common(12))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6298b2",
   "metadata": {},
   "source": [
    "#### **Prepare Data for Modelling**\n",
    "\n",
    "We may need to join tokens into strings for TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fdf055bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tweets = [\" \".join(tokens) for tokens in cleaned_tweets]  # Join tokens into strings\n",
    "\n",
    "X = cleaned_tweets\n",
    "y = hate_speech['label']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e40f4a",
   "metadata": {},
   "source": [
    "#### **TF-IDF Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8625bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff95728",
   "metadata": {},
   "source": [
    "#### **Logistic Regression Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a96622ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.955649419218585\n",
      "Train Recall: 0.955649419218585\n",
      "Train F1 Score: 0.9470729109073178\n"
     ]
    }
   ],
   "source": [
    "# First Model\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Prediction\n",
    "y_train_pred = lr.predict(X_train_tfidf)\n",
    "y_test_pred = lr.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Train Accuracy:\", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Train Recall:\" , recall_score(y_train, y_train_pred, average='weighted'))\n",
    "print(\"Train F1 Score:\", f1_score(y_train, y_train_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792f8f60",
   "metadata": {},
   "source": [
    "#### **Handle Class Imbalance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54a74b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Train Accuracy: 0.9425476162540577\n",
      "Balanced Train Recall: 0.9425476162540577\n",
      "Balanced Train F1 Score: 0.9495076676748813\n"
     ]
    }
   ],
   "source": [
    "lr_balanced =LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
    "lr_balanced.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_train_pred_balanced = lr_balanced.predict(X_train_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"Balanced Train Accuracy:\", accuracy_score(y_train, y_train_pred_balanced))\n",
    "print(\"Balanced Train Recall:\", recall_score(y_train, y_train_pred_balanced, average='weighted'))\n",
    "print(\"Balanced Train F1 Score:\", f1_score(y_train, y_train_pred_balanced, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062dbb4",
   "metadata": {},
   "source": [
    "#### **Hyperparameter Tuning with GridSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f307a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10], # Inverse of regularization strength\n",
    "    'penalty': ['l2'], # Regularization type , 'l1' requires solver='liblinear'\n",
    "    'solver': ['lbfgs', 'liblinear'] # Optimization algorithm\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42), # Base model\n",
    "    param_grid, # Hyperparameter grid\n",
    "    scoring='recall', # Evaluation metric\n",
    "    cv=skf, # Cross-validation strategy\n",
    "    n_jobs=-1, # Use all available cores\n",
    ")\n",
    "\n",
    "grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de54e5c5",
   "metadata": {},
   "source": [
    "#### **Final Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdcf0d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9206945096198967\n",
      "Test Recall: 0.7857142857142857\n",
      "Test F1 Score: 0.9299297047168323\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_best = best_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_test_pred_best))\n",
    "print(\"Test Recall:\", recall_score(y_test, y_test_pred_best))\n",
    "print(\"Test F1 Score:\", f1_score(y_test, y_test_pred_best, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d72c28",
   "metadata": {},
   "source": [
    "## üìå **Insights & Conclusion**\n",
    "\n",
    "- **Best Parameters:** Found using GridSearchCV with Stratified K-Fold cross-validation.\n",
    "- **Recall** is prioritised because we want to **catch as many hate tweets as possible**.\n",
    "- **TF-IDF** helped in representing tweets effectively.\n",
    "- **Class imbalance** was handled using `class_weight='balanced'`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
